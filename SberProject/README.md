### Проектная работа по курсу "MLOps"
#### Построение системы интеллектуального поиска по порталу СберСпасибо с применением технологий глубокого обучения
##### Автор: Кравченя Павел

##### Постановка задачи:

1. Реализовать сервис "умного" поиска, который в ответ на текстовый запрос клиента формирует поисковую выдачу по сайту https://new.spasibosberbank.ru/.  Поиск должен проводиться по объектам разделов сайта: купоны, акции, 
партнеры, впечатления.

2. Поиск должен осуществляться не только по прямому вхождению слов запроса в описание объектов, но и обобщать смысл запроса и искать схожие по смыслу объекты.

3. Архитектурно решение должно быть развернуто на кластере Kubernetes.

4. Необходимо продумать метрики качества поисковой выдачи, а также, при необходимости, решить проблему с некачественными текстовыми описаниями объектов.

##### Решение:

Решение задачи представлено в виде нескольких разработанных и / или настроенных сервисов, описанных ниже.

Дополнительная информация о разработанной системе приведена в аннотации и презентации к защите.

##### Векторная база данных Weaviate

База данных [Weaviate](https://weaviate.io/) используется для хранения эмбеддингов и эффективного поиска среди них по запросу. Данная база имеет возможность подключения модулей, расширяющих ее функкциональность. Модуль, использующийся в данной работе, позволяет выполнять расчет эмбеддингов для текстового описания ресурсов или запросов пользователей. Настройки базы и модулей приведены в файле `values.yaml`.

Для установки базы данных `Weaviate` и ее модулей необходимо выполнить следующие команды:

`$ cd weaviate/k8s/helm`

`$ kubectl create namespace weaviate`

`$ helm upgrade "weaviate" weaviate.tgz --install --namespace "weaviate" --values ./values.yaml --set "image.tag=1.14.1"`

Проверить работоспособность базы можно, выполнив проброс порта и обратившись по нему с HTTP-запросом:

`$ kubectl port-forward svc/weaviate 8080:80 -n weaviate`

`$ curl http://localhost:8080/v1/schema`

Работоспособность всех модулей можно посмотреть командой:

`$ kubectl get pods --namespace weaviate -w`

Удаление базы и сохраненных файлов в `pvc` можно последовательностью действий:

`$ helm uninstall weaviate -n weaviate`

`$ kubectl get pvc -n weaviate`

`$ kubectl delete pvc weaviate-data-weaviate-X -n weaviate`

`Замечание` В процессе тестирования работы данной базы выяснилось, что при количестве реплик, больше чем 1, работоспособность Weaviate сохраняется до первой перезагрузки. После нее база оказывается в неработоспособном состоянии, возвращая на любой запрос данных ошибку:

`{"error":[{"message":"list objects: search index sberentity: remote shard 1wysSdGnc53k: resolve node name \"weaviate-0\" to host"}]}`

В документации [сказано](https://weaviate.io/developers/weaviate/v1.8.0/more-resources/migration-guide.html), что эта ошибка может появляться при работе в локальном окружении при использовании `docker-compose`, но на `Kubernetes` ее быть не должно. Однако, запуск базы в `Kubernetes` показал обратное. Если запущена только одна реплика базы, она работает без ошибок.

##### Сервис парсинга `parsing-service`

Сервис `parsing-service` осуществляет парсинг содержимого нескольких разделов портала СберСпасибо и запускается в `Kubernetes` периодически по расписанию. Во время парсинга он передает обнаруженную информацию в Weaviate. Адрес и порт Weaviate-сервера передаются в сервис в процессе выполнения Airflow DAG как переменные окружения и задаются в файле `airflow/dags/parsing_dag.py`. 

Сборка образа с сервисом и его помещение в репозиторий осуществляется командами (перед выполнением команды нужно проверить наличие указанного в `docker-compose.yml`репозитория и, при отсутствии, создать его):

`$ cd parsing/docker`

`$ docker-compose build`

`$ docker-compose push`

Запуск сервиса по расписанию осуществляется с помощью инструмента `Apache Airflow`. Его образ которого необходимо собрать, включив в него информацию о DAG'е для `parsing-service`:

`$ cd airflow/docker`

`$ docker-compose build`

`$ docker-compose push`

Для запуска сервиса необходимо установить `Apache Airflow` в `Kubernetes`, добавив helm-репозиторий и сконфигурировав систему на использование требуемого образа:

`$ helm repo add apache-airflow https://airflow.apache.org`

`$ kubectl create namespace airflow`

`$ helm upgrade --install airflow apache-airflow/airflow --namespace airflow --set images.airflow.repository=cr.yandex/crpl2ivjm7kaokv219ge/airflow --set images.airflow.tag=latest --set images.airflow.pullPolicy=Always`

Мониторинг состояния работы подов можно осуществлять командой:

`kubectl get pods --namespace airflow -w`

Удалить `Airflow` с кластера `Kubernetes` возможно следующим способом:

`$ helm uninstall airflow -n airflow`

`$ kubectl get pvc -n airflow`

`$ kubectl delete pvc data-airflow-postgresql-0 -n airflow`

`$ kubectl delete pvc logs-airflow-worker-0 -n airflow`

`$ kubectl delete pvc redis-db-airflow-redis-0 -n airflow`

`Замечание` `Yandex Cloud` предоставляет только два [сетевых балансировщика нагрузки](https://cloud.yandex.ru/docs/network-load-balancer/) по умолчанию. Для увеличения их количества следует обратиться с запросом к службе поддержки.

##### Сервис поиска `search-service`

Сервис предназначен для получения запросов на поиск от пользователя, переадресации этого запроса с дополнительными параметрами базе данных `Weaviate`, получения ответа от нее и формирования ответа клиенту. Для запуска сервиса необходимо наличичие работающей базы данных Weaviate. Информация для подключения к ней (адрес и порт) передается через переменные окружения, прописанные в файле `search-deployment.yml`. Для дальнейшего запуска сервиса необходимо собрать соответствующий Docker-образ и поместить его в репозиторий:

`$ cd search/docker`

`$ docker-compose build`

`$ docker-compose push`

Запуск сервиса в `Kubernetes` осуществляется следующей последовательностью действий:

`$ cd search/k8s`

`$ kubectl apply -f search-deployment.yml`

`$ kubectl apply -f search-service.yml`

`$ kubectl apply -f search-ingress.yml`

Удалить сервис с кластера `Kubernetes` можно следующим способом:

`$ cd search/k8s`

`$ kubectl delete -f search-deployment.yml`

`$ kubectl delete -f search-service.yml`

`$ kubectl delete -f search-ingress.yml`

##### Сервис расчета метрик поиска `metrics-service`

Одной из задач сервиса метрик является сбор информации о пользовательских запросах, возвращаемых системой ответах и реакции пользователя на них (наличие или отсутствие кликов по ссылкам). Для хранения полученной информации используется база данных MongoDB, которую необходимо подготовить и запустить заранее. При создании базы в MongoDB нужно указать ее имя: `search_results`, а также задать имя и пароль пользователя. Имя и пароль необходимо поместить в объект `secrets Kubernetes`, создав файл `secret.yaml` со следующим содержимым:

    apiVersion: v1
    kind: Secret
    metadata:
        name: mongosecret
        namespace: metrics
    type: Opaque
    data:
        username: <имя пользователя MongoDB в base64>
        password: <пароль MongoDB в base64>

и выполнив команду по добавлению секрета в `Kubernetes`:

`$ kubectl apply -f secret.yaml`

Для запуска сервиса в `Kubernetes` необходимо сначала собрать его образ и поместить его в хранилище:

`$ cd metrics/docker`

`$ docker-compose build`

`$ docker-compose push`

а затем выполнить команды по разворачиванию:

`$ cd metrics/k8s`

`$ kubectl apply -f metrics-deployment.yml`

`$ kubectl apply -f metrics-service.yml`

`$ kubectl apply -f metrics-ingress.yml`

Сам сервис не содержит данных, размещаемых в `pvc`, поэтому для его удаления достаточно выполнить следующие команды:

`$ kubectl delete -f metrics-deployment.yml`

`$ kubectl delete -f metrics-service.yml`

`$ kubectl delete -f metrics-ingress.yml`

##### Сервис веб-интерфейса `frontend-service`

Данный сервис предоставляет пользователю графический интерфейс в виде веб-страницы, на которой размещены элементы по отправке запросов поиска и отображения его результатов. Данная информация предоставляется при обращении браузером по адресу размещения сервиса. Также, этот сервис прозрачно для пользователя отправляет статистическую информацию о предпочтениях клиента на `metrics-service`.

Для корректной работы сервиса ему необходимо предоставить информацию об адресах сервисов `search-service` и `metrics-service`. Они прописываются в переменные окружения файла `frontend/docker/Dockerfile`.

Для запуска сервиса в `Kubernetes` необходимо собрать его Docker-образ, поместить его в репозиторий и развернуть. Команды, выполняющие эти операции, аналогичны таковым для вышерассмотренных сервисов.

##### Выводы:

В процессе выполнения работы была спроектирована, разработана, развернута в `Yandex Managed Service for Kubernetes` и протестирована система поиска пользовательских предпочтений по ресурсам портала СберСпасибо. Реализован и развернут сервис сбора информации о пользовательских кликах и расчета метрик на ее основе.
