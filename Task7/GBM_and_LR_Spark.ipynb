{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание № 7 по курсу \"MLOps\"\n",
    "#### Обучение сложных моделей машинного обучения с помощью распределенных инструментов\n",
    "##### Автор: Кравченя Павел\n",
    "\n",
    "##### Цели работы:\n",
    "Тренировка обучения модель градиентного бустинга в распределенном режиме с помощью Spark.\n",
    "\n",
    "##### Постановка задачи:\n",
    "\n",
    "1. Скачать [датасет](https://www.kaggle.com/sharthz23/sna-hackathon-2019-collaboration/download). В нем содержатся табличные данные показов ленты социальной сети ``ok.ru`` за 1,5 месяца. Для анализа следует использовать данные, расположенные в директории ``sna-hackathon-2019/train``.\n",
    "\n",
    "2. Подготовить признаки на основе датасета.\n",
    "\n",
    "3. Обучить модель градиентного бустинга на Spark.\n",
    "\n",
    "4. Оценить качество модели.\n",
    "\n",
    "Работа выполнялась с использованием Docker-образа системы ``almond.sh`` с версией ``Scala 2.12`` и ``Spark 3.1.0``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Сборка Docker-образа\n",
    "Поскольку для работы градиентного бустинга требовалась библиотека OpenMP, которая отсутствовала в Docker-образе ``almond.sh``, было принято решение собрать на его основе новый образ, доустановив библиотеку. Последовательность действий для сборки образа и запуска проекта:\n",
    "\n",
    "``git clone https://github.com/kpdphys/MLOps.git``\n",
    "\n",
    "``cd MLOps/Task5``\n",
    "\n",
    "``docker build -t openmp-almondsh:latest ./docker``\n",
    "\n",
    "``docker run -p 8888:8888 --rm -v $(pwd):/home/jovyan/work --name openmp-almondsh openmp-almondsh:latest``\n",
    "\n",
    "После успешных сборки образа и запуска контейнера веб-интерфейс ``jupyter notebook (Almondsh)`` будет доступен на порту ``8888``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выполнение работы\n",
    "\n",
    "`Замечание` Поскольку работа с градиентным бустингом на Spark уже осуществлялась автором в пятом домашнем задании, а с нейронной сетью -- в третьем, то, **по согласованию с руководителем курса**, было решено сделать следующее. К уже имеющемуся анализу датасета, который осуществлялся с использованием градиентного бустинга, добавить анализ с применением простой линейной модели (логистической регрессии) и сравнить результаты. Поэтому, первая часть работы будет повторять содержание пятого домашнего задания.\n",
    "\n",
    "Для построения модели и выполнения ее интерпретации воспользуемся библиотекой Microsoft [SynapseML](https://microsoft.github.io/SynapseML). Установим ее и другие необходимые библиотеки для выполнения вычислений и визуализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:3.1.0`\n",
    "import $ivy.`com.microsoft.azure::synapseml:0.9.5`\n",
    "import $ivy.`org.plotly-scala::plotly-almond:0.7.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import plotly._, plotly.element._, plotly.layout._, plotly.Almond._\n",
    "\n",
    "import org.apache.spark.sql.{DataFrame, Dataset, SparkSession}\n",
    "import org.apache.spark.ml.{Pipeline, PipelineModel}\n",
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "import org.apache.spark.ml.feature.{Imputer, OneHotEncoder, StringIndexer, VectorAssembler, StandardScaler}\n",
    "import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n",
    "import com.microsoft.azure.synapse.ml.lightgbm._\n",
    "import org.apache.spark.ml.functions.vector_to_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим необходимый уровень логирования сообщений и создадим Spark-сессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.log4j.{Level, Logger}\n",
    "import org.apache.spark.sql._\n",
    "Logger.getLogger(\"org\").setLevel(Level.OFF)\n",
    "\n",
    "val spark = {\n",
    "  NotebookSparkSession.builder()\n",
    "    .master(\"local[*]\")\n",
    "    .config(\"spark.executor.memory\", \"32g\")\n",
    "    .config(\"spark.driver.memory\", \"32g\")\n",
    "    .getOrCreate()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим датасет и выведем схему данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  val data = spark.read.parquet(\"sna-hackathon-2019/train\")\n",
    "  data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как датасет содержит большое количество признаков, выберем из них те, значения которых вероятнее всего будут влиять на отток пользователей. Вместе с названиями отобранных признаков сохраним их тип (``категориальные``('c') или ``количественные`` ('q')), поскольку от него будет зависеть преобработка данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val features_types = Map(\n",
    "    \"instanceId_objectType\" -> 'c',\n",
    "    \"audit_clientType\" -> 'c',\n",
    "    \"audit_resourceType\" -> 'c',\n",
    "    \"membership_status\" -> 'c',\n",
    "    \"user_gender\" -> 'c',\n",
    "    \"user_status\" -> 'c',\n",
    "    \"user_ID_country\" -> 'c',\n",
    "    \"metadata_ownerType\" -> 'c',\n",
    "    \"metadata_platform\" -> 'c',\n",
    "    \"userOwnerCounters_USER_PROFILE_VIEW\" -> 'c',\n",
    "    \"userOwnerCounters_USER_SEND_MESSAGE\" -> 'c',\n",
    "    \"userOwnerCounters_USER_INTERNAL_LIKE\" -> 'c',\n",
    "    \"userOwnerCounters_USER_STATUS_COMMENT_CREATE\" -> 'c',\n",
    "    \"userOwnerCounters_USER_FORUM_MESSAGE_CREATE\" -> 'c',\n",
    "    \"userOwnerCounters_PHOTO_COMMENT_CREATE\" -> 'c',\n",
    "    \"userOwnerCounters_COMMENT_INTERNAL_LIKE\" -> 'c',\n",
    "    \"userOwnerCounters_MOVIE_COMMENT_CREATE\" -> 'c',\n",
    "    \"userOwnerCounters_USER_PHOTO_ALBUM_COMMENT_CREATE\" -> 'c',\n",
    "\n",
    "    \"auditweights_userAge\" -> 'q',\n",
    "    \"metadata_numCompanions\" -> 'q',\n",
    "    \"userOwnerCounters_CREATE_LIKE\" -> 'q',\n",
    "    \"auditweights_ageMs\" -> 'q',\n",
    "    \"auditweights_ctr_gender\" -> 'q',\n",
    "    \"auditweights_ctr_high\" -> 'q',\n",
    "    \"auditweights_ctr_negative\" -> 'q',\n",
    "    \"auditweights_dailyRecency\" -> 'q',\n",
    "    \"auditweights_feedStats\" -> 'q',\n",
    "    \"auditweights_friendCommentFeeds\" -> 'q',\n",
    "    \"auditweights_friendCommenters\" -> 'q',\n",
    "    \"auditweights_friendLikes\" -> 'q'\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим пользователей как ушедших (``churn``), если они не совершали никаких действий с системой **в течение двух недель**. Для этого определим текущее на момент формирования датасета время как время самого последнего события в системе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.functions._\n",
    "val curr_timestamp = data.select(\"audit_timestamp\").agg(max(\"audit_timestamp\")).take(1)(0).getLong(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всех пользователей, для которых последняя запись о любом их взаимодействии с системой обнаруживается раньше, чем за две недели до вычисленной даты, разметим как ушедших ``(is_churn)``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._\n",
    "val wait_time_days: Int = 14\n",
    "val labeled_data = data.select(\"instanceId_userId\", \"audit_timestamp\")\n",
    "    .groupBy(\"instanceId_userId\")\n",
    "    .agg(max(\"audit_timestamp\")\n",
    "      .as(\"last_att_timestamp\"))\n",
    "    .withColumn(\"curr_timestamp\", lit(curr_timestamp))\n",
    "    .withColumn(\"delta_timestamp\", $\"curr_timestamp\" - $\"last_att_timestamp\")\n",
    "    .withColumn(\"days_absent\", $\"delta_timestamp\" / (24 * 3600 * 1000))\n",
    "    .withColumn(\"is_churn\", when($\"days_absent\" > wait_time_days, 1.0).otherwise(0.0))\n",
    "\n",
    "labeled_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим метку к каждому пользователю для каждой записи в датасете и отфильтруем признаки, оставив только ранее выбранные."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val clean_labeled_data = data\n",
    "    .join(labeled_data, data(\"instanceId_userId\") === labeled_data(\"instanceId_userId\"), \"left\")\n",
    "    .select((features_types.toArray.map(x => col(x._1)) :+ col(\"is_churn\")): _*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим количества записей в датасете отдельно для ушедших и оставшихся пользователей, а также их отношение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val churn_count = clean_labeled_data.filter($\"is_churn\" === 1.0).count\n",
    "val non_churn_count = clean_labeled_data.filter($\"is_churn\" === 0.0).count\n",
    "val churn_ratio = non_churn_count.toDouble / churn_count\n",
    "println(f\"churn_count = $churn_count, non_churn_count = $non_churn_count, ratio = $churn_ratio%.3f\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что данных, имеющих отношение к ушедшим пользователям, меньше в 4 раза, чем к оставшимся. Выполним процедуру ``undersampling`` для балансировки датасета перед обучением."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def underample_df(df: DataFrame): DataFrame = {\n",
    "    val churn_df = df.filter($\"is_churn\" === 1.0)\n",
    "    val non_churn_df = df.filter($\"is_churn\" === 0.0)\n",
    "    val sample_ratio = churn_df.count().toDouble / df.count().toDouble\n",
    "    val non_churn_sampled_Df = non_churn_df.sample(false, sample_ratio)\n",
    "    churn_df.union(non_churn_sampled_Df)\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разобьем стратифицированно (т.е. с соблюдением баланса классов) датасет на обучающую и тестовую выборки в пропорции ``70:30``. Перемешаем полученные выборки для более равномерного обучения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val splitted_data = clean_labeled_data.randomSplit(Array(0.7, 0.3), seed = 1234L)\n",
    "val train_dataset = underample_df(splitted_data(0)).orderBy(rand())\n",
    "val test_dataset = splitted_data(1).orderBy(rand())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим модель на основе градиентного бустинга над деревьями решений. Реализацию алгоритма бустинга возьмем из библиотеки ``LightGBM``, которая является составной частью ``SynapseML``. При создании классификатора укажем столбец результирующего датасета, в который будут записаны коэффициенты Шепли, необходимые для интерпретации обученной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val clsfr_gbm = new LightGBMClassifier()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"is_churn\")\n",
    "    .setObjective(\"binary\")\n",
    "    .setFeaturesShapCol(\"shapValues\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим конвейер данных, в котором перед обучением классификатора категориальные данные будут кодироваться посредством ``LabelEncoding``, а в количественных данных будут заполнены пропуски."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline_gbm = new Pipeline()\n",
    "    .setStages(Array(\n",
    "      new StringIndexer()\n",
    "        .setHandleInvalid(\"keep\")\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1))\n",
    "        .setOutputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ind\")),\n",
    "      new Imputer()\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1))\n",
    "        .setOutputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1 + \"_imp\")),\n",
    "      new VectorAssembler()\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1 + \"_imp\") ++ \n",
    "                      features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ind\"))\n",
    "        .setOutputCol(\"features\"),\n",
    "      clsfr_gbm\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model_gbm = pipeline_gbm.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Определим предсказания обученной модели на тренировочной и тестовой части датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val train_predictions_gbm = model_gbm.transform(train_dataset)\n",
    "val test_predictions_gbm  = model_gbm.transform(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для оценки качества модели воспользуемся объектом ``BinaryClassificationEvaluator``, который вычислит значения ``AUC ROC`` для данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "val evaluator = new BinaryClassificationEvaluator()\n",
    "    .setLabelCol(\"is_churn\")\n",
    "    .setMetricName(\"areaUnderROC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем рассчитанные значения ``AUC ROC`` для тренировочной и тестовой частей датасета."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val areaUnderROC_train_gbm = evaluator.evaluate(train_predictions_gbm)\n",
    "val areaUnderROC_test_gbm  = evaluator.evaluate(test_predictions_gbm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получившееся значение ~67% как на тренировочной, так и на тестой частях датасета является небольшим, но его можно признать удовлетворительным для Baseline-модели. Рассчитаем также другие метрики классификации на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.microsoft.azure.synapse.ml.train.ComputeModelStatistics\n",
    "val stats = new ComputeModelStatistics()\n",
    "    .setLabelCol(\"is_churn\")\n",
    "    .setScoresCol(\"probability_1\")\n",
    "    .setScoredLabelsCol(\"prediction\")\n",
    "    .setEvaluationMetric(\"classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.functions.vector_to_array\n",
    "val metrics_gbm = stats.transform(test_predictions_gbm.\n",
    "                                  withColumn(\"probability_1\", \n",
    "                                             vector_to_array($\"probability\").getItem(1)))\n",
    "metrics_gbm.select(\"accuracy\", \"precision\", \"recall\", \"AUC\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили Acc = ~64%, Pr = ~64%, Rc = ~82%. Аналогично вышерассмотренному случаю, метрики можно считать удовлетворительными для Baseline-модели.\n",
    "\n",
    "`Замечание` При разных запусках расчета метрик для градиентного бустинга результаты могут получаться существенно разными. Выше приведены наиболее высокие значения, которых удалось добиться. Причины такого поведения пока не выяснены.\n",
    "\n",
    "Сформируем небольшую выборку из тестовой части датасета, состоящую из объектов, на которых модель выдала максимальную вероятность ухода. Они соответствуют пользователям, которые, по мнению модели, уйдут в самое ближайшее время. Именно для этих объектов попробуем объяснить влияние на вероятность ухода клиента различных признаков, на которых обучалась модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val explain_instances_gbm = test_predictions_gbm\n",
    "    .filter($\"prediction\" === 1.0)\n",
    "    .withColumn(\"probability_1\", vector_to_array($\"probability\").getItem(1))\n",
    "    .orderBy($\"probability_1\".desc)\n",
    "    .limit(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сформируем функцию, которая получает значения коэффициентов Шепли для рассматриваемых объектов, усредняет их и визуализирует несколько самых больших по модулю значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_gbm(shap_data: Dataset[_], top_obj_count: Int): Unit = {\n",
    "    import org.apache.spark.ml.linalg.DenseVector\n",
    "    import org.apache.spark.ml.stat.Summarizer\n",
    "    \n",
    "    def get_features_from_dataframe(data: Dataset[_]): Array[String] = {\n",
    "        val meta = data\n",
    "            .schema(\"features\")\n",
    "            .metadata\n",
    "            .getMetadata(\"ml_attr\")\n",
    "            .getMetadata(\"attrs\")\n",
    "\n",
    "        val meta_numeric = if(meta.contains(\"numeric\")) {\n",
    "            meta\n",
    "            .getMetadataArray(\"numeric\")\n",
    "            .map(x => (x.getLong(\"idx\"), x.getString(\"name\")))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        val meta_nominal = if(meta.contains(\"nominal\")) {\n",
    "            meta\n",
    "            .getMetadataArray(\"nominal\")\n",
    "            .map(x => (x.getLong(\"idx\"), x.getString(\"name\")))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        val meta_binary = if(meta.contains(\"binary\")) {\n",
    "            meta\n",
    "            .getMetadataArray(\"binary\")\n",
    "            .map(x => (x.getLong(\"idx\"), x.getString(\"name\")))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        (meta_numeric ++ meta_nominal ++ meta_binary).sortBy(_._1).map(_._2)\n",
    "    }\n",
    "    \n",
    "    val shaps = shap_data\n",
    "        .select(\"shapValues\")\n",
    "        .groupBy()\n",
    "        .agg(Summarizer.mean($\"shapValues\").alias(\"means\"))\n",
    "        .map { case Row(shapValues_1: DenseVector) => shapValues_1.toArray } collect\n",
    "    \n",
    "    val shaps_with_features = shaps(0)\n",
    "        .drop(1)\n",
    "        .zip(get_features_from_dataframe(shap_data))\n",
    "        .sortBy(_._1)\n",
    "    \n",
    "    val filtered_shaps_with_features = shaps_with_features.slice(0, top_obj_count) ++\n",
    "        shaps_with_features.slice(shaps_with_features.size - top_obj_count, \n",
    "                                  shaps_with_features.size)\n",
    "    \n",
    "    filtered_shaps_with_features.foreach(x => println(x._2))\n",
    "    \n",
    "    val data = Seq(Bar(\n",
    "        filtered_shaps_with_features.map(_._1).toSeq,\n",
    "        filtered_shaps_with_features.map(_._2).toSeq,\n",
    "        orientation = Orientation.Horizontal\n",
    "    ))\n",
    "    \n",
    "    plot(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выполним визуализацию усредненных коэффициентов Шепли для клиентов, вероятность которых уйти, согласно обученной модели, наибольшая."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_gbm(explain_instances_gbm, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из гистограммы можно наблюдать, что самое большое влияние на вероятность ухода оказали следующие признаки:\n",
    "* ``user_status`` (статус пользователя)\n",
    "* ``auditweights_ctr_negative`` (рейтинг кликов пользователя)\n",
    "* ``user_gender`` (пол пользователя)\n",
    "* ``membership_status`` (информация о членстве пользователя в группе, где опубликован контент)\n",
    "\n",
    "Таким образом, статус пользователя непосредственно влияет на вероятность ухода. Возможно, данный признак непосредственно рассчитывается с учетом ушедших клиентов, и тогда является утечкой данных при анализе :) Сказать заранее нельзя, поскольку алгоритм его расчета неизвестен.\n",
    "\n",
    "Рейтинг кликов и членство пользователя в группах показывают заинтересованность пользователя в контенте социальной сети. Следует предложить бизнес-аналитикам продумать способы привлечения внимания новых клиентов к предоставляемому им материалу, что, возможно, снизит риск оттока пользователей.\n",
    "\n",
    "С учетом того, что пол клиента имеет большое влияние на вероятность оттока клиентов, следует порекомендовать бизнес-аналитикам проанализировать материал на ориентированность только на мужскую или женскую аудиторию. Также, следует исключить возможные варианты сексизма в контенте пользователей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуем проанализировать признаки, оказывающие большое влияние на уход пользователя, с помощью логистической регрессии. Дальшнейшие шаги будут схожи с вышеописанными для градиентного бустинга."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegression\n",
    "val clsfr_lr = new LogisticRegression()\n",
    "    .setFeaturesCol(\"features\")\n",
    "    .setLabelCol(\"is_churn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для линейных моделей количественные признаки необходимо отмасштабировать, а категориальные -- закодировать с помощью `one-hot`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val pipeline_lr = new Pipeline()\n",
    "    .setStages(Array(\n",
    "      new StringIndexer()\n",
    "        .setHandleInvalid(\"keep\")\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1))\n",
    "        .setOutputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ind\")),\n",
    "      \n",
    "      new OneHotEncoder()\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ind\"))\n",
    "        .setOutputCols(features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ohe\")),\n",
    "        \n",
    "      new Imputer()\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1))\n",
    "        .setOutputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1 + \"_imp\")),\n",
    "        \n",
    "      new VectorAssembler()\n",
    "        .setInputCols(features_types.filter(x => x._2 == 'q').toArray.map(x => x._1 + \"_imp\"))\n",
    "        .setOutputCol(\"q_imp\"),  \n",
    "        \n",
    "      new StandardScaler()\n",
    "        .setInputCol(\"q_imp\")\n",
    "        .setOutputCol(\"q_norm\"),\n",
    "\n",
    "      new VectorAssembler()\n",
    "        .setInputCols(Array(\"q_norm\") ++ features_types.filter(x => x._2 == 'c').toArray.map(x => x._1 + \"_ohe\"))\n",
    "        .setOutputCol(\"features\"),\n",
    "    \n",
    "      clsfr_lr\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val model_lr = pipeline_lr.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val train_predictions_lr = model_lr.transform(train_dataset)\n",
    "val test_predictions_lr = model_lr.transform(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассчитаем значения AUC ROC для линейной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val areaUnderROC_train_lr = evaluator.evaluate(train_predictions_lr)\n",
    "val areaUnderROC_test_lr  = evaluator.evaluate(test_predictions_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И train, и test показали AUC ROC ~65%. Это примерно на 2% меньше результатов, достигнутых с использованием градиентного бустинга. Как говорилось ранее, для baseline-модели это -- удовлетворительное значение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val metrics_lr = stats.transform(test_predictions_lr\n",
    "                                 .withColumn(\"probability_1\", \n",
    "                                             vector_to_array($\"probability\").getItem(1)))\n",
    "metrics_lr.select(\"accuracy\", \"precision\", \"recall\", \"AUC\").show(false)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили долю верных ответов (accuracy) ~62%, точность (precision) ~63%, полноту (recall) ~76%. Результаты схожи с аналогичными данными, полученными с использованием градиентного бустинга.\n",
    "\n",
    "Вновь отберем 40 пользователей, вероятность ухода которых, согласно модели, максимальна."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val explain_instances_lr = test_predictions_lr\n",
    "    .filter($\"prediction\" === 1.0)\n",
    "    .withColumn(\"probability_1\", vector_to_array($\"probability\").getItem(1))\n",
    "    .orderBy($\"probability_1\".desc)\n",
    "    .limit(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При обучении модели градиентного бустинга с использованием библиотеки `SynapseML` коээфициенты Шепли считались библиотекой во время обучения. При использовании модели логистической регрессии эти коэффициенты необходимо посчитать отдельно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.ml.classification.LogisticRegressionModel\n",
    "import com.microsoft.azure.synapse.ml.explainers.VectorSHAP\n",
    "\n",
    "val shap_lr = new VectorSHAP()\n",
    "  .setInputCol(\"features\")\n",
    "  .setOutputCol(\"shapValues_Vector\")\n",
    "  .setNumSamples(5000)\n",
    "  .setModel(model_lr.stages.last.asInstanceOf[LogisticRegressionModel])\n",
    "  .setTargetCol(\"probability\")\n",
    "  .setTargetClasses(Array(1))\n",
    "  .setBackgroundData(test_predictions_lr.orderBy(rand()).limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shap_df_lr = shap_lr.transform(explain_instances_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Преобразуем датасет путем извлечения из полученного вектора нужных коэффициентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val shap_values_df_lr = shap_df_lr.withColumn(\"shapValues\", $\"shapValues_Vector\".getItem(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_shap_lr(shap_data: Dataset[_], \n",
    "                 features_types: Map[String, Char], \n",
    "                 top_obj_count: Int): Unit = {\n",
    "    import org.apache.spark.ml.linalg.DenseVector\n",
    "    import org.apache.spark.ml.stat.Summarizer\n",
    "    \n",
    "    def get_features_from_dataframe(data: Dataset[_]): Array[String] = {\n",
    "        val meta = data\n",
    "            .schema(\"features\")\n",
    "            .metadata\n",
    "            .getMetadata(\"ml_attr\")\n",
    "            .getMetadata(\"attrs\")\n",
    "\n",
    "        val meta_numeric = if(meta.contains(\"numeric\")) {\n",
    "            val q_features = features_types\n",
    "                .filter(x => x._2 == 'q')\n",
    "                .toArray\n",
    "                .map(x => x._1)\n",
    "                .zipWithIndex\n",
    "                .map(x => (\"q_norm_\" + x._2 -> x._1))\n",
    "                .toMap\n",
    "            meta\n",
    "            .getMetadataArray(\"numeric\")\n",
    "            .map(x => (x.getLong(\"idx\"), q_features(x.getString(\"name\"))))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        val meta_nominal = if(meta.contains(\"nominal\")) {\n",
    "            meta\n",
    "            .getMetadataArray(\"nominal\")\n",
    "            .map(x => (x.getLong(\"idx\"), x.getString(\"name\")))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        val meta_binary = if(meta.contains(\"binary\")) {\n",
    "            meta\n",
    "            .getMetadataArray(\"binary\")\n",
    "            .map(x => (x.getLong(\"idx\"), x.getString(\"name\")))\n",
    "        } else { Array[(Long, String)]() }\n",
    "    \n",
    "        (meta_numeric ++ meta_nominal ++ meta_binary).sortBy(_._1).map(_._2)\n",
    "    }\n",
    "    \n",
    "    val shaps = shap_data\n",
    "        .select(\"shapValues\")\n",
    "        .groupBy()\n",
    "        .agg(Summarizer.mean($\"shapValues\").alias(\"means\"))\n",
    "        .map { case Row(shapValues_1: DenseVector) => shapValues_1.toArray } collect\n",
    "    \n",
    "    val shaps_with_features = shaps(0)\n",
    "        .drop(1)\n",
    "        .zip(get_features_from_dataframe(shap_data))\n",
    "        .sortBy(_._1)\n",
    "    \n",
    "    val filtered_shaps_with_features = shaps_with_features.slice(0, top_obj_count) ++\n",
    "        shaps_with_features.slice(shaps_with_features.size - top_obj_count, \n",
    "                                  shaps_with_features.size)\n",
    "    \n",
    "    filtered_shaps_with_features.foreach(x => println(x._2))\n",
    "    \n",
    "    val data = Seq(Bar(\n",
    "        filtered_shaps_with_features.map(_._1).toSeq,\n",
    "        filtered_shaps_with_features.map(_._2).toSeq,\n",
    "        orientation = Orientation.Horizontal\n",
    "    ))\n",
    "    \n",
    "    plot(data)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_shap_lr(shap_values_df_lr, features_types, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Согласно линейной модели, наибольшее влияние на уход пользователя оказывают:\n",
    "1. Признак `auditweights_dailyRecency`, который, судя по названию, непосредственно связан с ежедневной активностью пользователя.\n",
    "2. Признак `auditweights_ageMs`, который, возможно, связан с возрастом пользователя (сложно понять из названия).\n",
    "\n",
    "Интересной особенностью является тот факт, что признаки, отобранные логистической регрессией и градиентным бустингом, в большинстве своем не совпадают. Только признак, коррелирующий с возрастом пользователя, фигурирует в результатах обеих моделей.\n",
    "\n",
    "Также, была выявлена следующая особенность. При различных запусках обучения модели и валидации результатов `lightgbm` выдает метрики, которые могут значительно отличаться. При этом, метрики, полученные с помощью логистической регрессии, практически не изменяются. Объяснения этому факту пока не найдено..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы:\n",
    "В процессе выполнения работы был проанализирован датасет показов ленты социальной сети ok.ru за 1,5 месяца с целью определения признаков, оказывающих наибольшее влияние на отток пользователей. Был выбран критерий оттока, с учетом которого датасет был размечен. Над размеченным датасетом решалась задача бинарной классификации (``churn``/ ``not churn``) с помощью классификаторов на основе градиентного бустинга и логистической регрессии. Проверка качества обученной модели показала, что метрики классификации можно считать удовлетворительными для Baseline-модели. Степень влияния различных признаков на вероятность оттока была проанализирована с помощью коэффициентов Шепли, которые были рассчитаны с использованием библиотеки ``SynapseML`` и визуализированы. По результатам анализа были сформулированы способы удержания наиболее серьезно настроенных уйти пользователей.\n",
    "\n",
    "Результаты и метрики, демонстрируемые линейной моделью, оказались схожими с результатами градиентного бустинга. Однако, признаки, которые две модели считают наиболее влияющими на готовность пользователей уйти, различаются.\n",
    "\n",
    "В результате работы были получены навыки работы с библиотекой ``SynapseML`` под ``Scala``, изучен способ интерпретации моделей машинного обучения с помощью коэффициентов Шепли."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala 2.12",
   "language": "scala",
   "name": "scala212"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".sc",
   "mimetype": "text/x-scala",
   "name": "scala",
   "nbconvert_exporter": "script",
   "version": "2.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
